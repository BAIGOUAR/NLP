{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python\n",
    "\n",
    "__author__=\"Daniel Bauer <bauer@cs.columbia.edu>\"\n",
    "__date__ =\"$Sep 12, 2011\"\n",
    "\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "Count n-gram frequencies in a data file and write counts to\n",
    "stdout. \n",
    "\"\"\"\n",
    "\n",
    "def simple_conll_corpus_iterator(corpus_file):\n",
    "    \"\"\"\n",
    "    Get an iterator object over the corpus file. The elements of the\n",
    "    iterator contain (word, ne_tag) tuples. Blank lines, indicating\n",
    "    sentence boundaries return (None, None).\n",
    "    \"\"\"\n",
    "    l = corpus_file.readline()\n",
    "    while l:\n",
    "        line = l.strip()\n",
    "        if line: # Nonempty line\n",
    "            fields = line.split(\" \")\n",
    "            ne_tag = fields[-1] # tag\n",
    "            word = \" \".join(fields[:-1])\n",
    "            yield word, ne_tag\n",
    "        else: # Empty line\n",
    "            yield (None, None)                        \n",
    "        l = corpus_file.readline()\n",
    "\n",
    "def sentence_iterator(corpus_iterator):\n",
    "    \"\"\"\n",
    "    Return an iterator object that yields one sentence at a time.\n",
    "    Sentences are represented as lists of (word, ne_tag) tuples.\n",
    "    \"\"\"\n",
    "    current_sentence = [] #Buffer for the current sentence\n",
    "    for l in corpus_iterator:    \n",
    "#             print (\"***NOW\",l) # Every line (every word) l -->  ('nucleotidase', 'I-GENE')\n",
    "            if l==(None, None):\n",
    "                if current_sentence:  #Reached the end of a sentence\n",
    "                    yield current_sentence\n",
    "                    current_sentence = [] #Reset buffer\n",
    "                else: # Got empty input stream\n",
    "                    sys.stderr.write(\"WARNING: Got empty input file/stream.\\n\")\n",
    "                    raise StopIteration\n",
    "            else:\n",
    "                current_sentence.append(l) #Add token to the buffer\n",
    "\n",
    "    if current_sentence: # If the last line was blank, we're done\n",
    "#         print (\"Current \",current_sentence)\n",
    "        yield current_sentence  #Otherwise when there is no more token\n",
    "                                # in the stream return the last sentence.\n",
    "        #Current_sentence  [('When', 'O'), ('CSF', 'O'), ('joj', 'O'),...]  (read in as a key-value pairs collection)\n",
    "\n",
    "def get_ngrams(sent_iterator, n):\n",
    "    \"\"\"\n",
    "    Get a generator that returns n-grams over the entire corpus,\n",
    "    respecting sentence boundaries and inserting boundary tokens.\n",
    "    Sent_iterator is a generator object whose elements are lists\n",
    "    of tokens. 每个句子前追加2个*，后追加一个STOP标志(w_boundary)\n",
    "    \"\"\"\n",
    "    for sent in sent_iterator:\n",
    "         # sent (list) --> [('omparison', 'O'), ('with', 'O'),  ...] (Tupels in every sentence)\n",
    "         #Add boundary symbols to the sentence\n",
    "         w_boundary = (n-1) * [(None, \"*\")]\n",
    "         w_boundary.extend(sent)\n",
    "         w_boundary.append((None, \"STOP\"))\n",
    "         #Then extract n-grams\n",
    "        \n",
    "         ngrams = (tuple(w_boundary[i:i+n]) for i in range(len(w_boundary)-n+1))\n",
    "         for n_gram in ngrams: #Return one n-gram at a time\n",
    "#             print (n_gram)\n",
    "            yield n_gram      \n",
    "\n",
    "#         ngrams\n",
    "#         (1, 2, 3)\n",
    "#         (2, 3, 4)\n",
    "#         (3, 4, 5)\n",
    "#         (4, 5, 6)\n",
    "#         (5, 6, 7)\n",
    "#         >>> 类似这种\n",
    "#  返回的是这样：\n",
    "# ((None, '*'), (None, '*'), ('omparison', 'O'))\n",
    "# ((None, '*'), ('omparison', 'O'), ('with', 'O'))\n",
    "# (('omparison', 'O'), ('with', 'O'), ('alkaline', 'I-GENE'))\n",
    "\n",
    "\n",
    "class Hmm(object):\n",
    "    \"\"\"\n",
    "    Stores counts for n-grams and emissions. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=3):\n",
    "        assert n>=2, \"Expecting n>=2.\"\n",
    "        self.n = n\n",
    "        self.emission_counts = defaultdict(int)\n",
    "        self.ngram_counts = [defaultdict(int) for i in range(self.n)]\n",
    "        self.all_states = set()\n",
    "#       stores emmision para: same type with counts. shape (word,tag):value   meaning em (word|tag)\n",
    "        self.emision_parameters = defaultdict(float)\n",
    "#       rare word list\n",
    "        self.rare = list()\n",
    "#       argmax word list :    word([tag,em],[tag,em],...)\n",
    "        self.argmax_list = defaultdict(list)\n",
    "        self.argmax_value = defaultdict(str)\n",
    "\n",
    "\n",
    "    def train(self, corpus_file):\n",
    "        \"\"\"\n",
    "        Count n-gram frequencies and emission probabilities from a corpus file.\n",
    "        \"\"\"\n",
    "        ngram_iterator = \\\n",
    "            get_ngrams(sentence_iterator(simple_conll_corpus_iterator(corpus_file)), self.n)\n",
    "        \n",
    "\n",
    "        for ngram in ngram_iterator:\n",
    "            #Sanity check: n-gram we get from the corpus stream needs to have the right length\n",
    "            assert len(ngram) == self.n, \"ngram in stream is %i, expected %i\" % (len(ngram, self.n))\n",
    "\n",
    "            tagsonly = tuple([ne_tag for word, ne_tag in ngram]) #retrieve only the tags    \n",
    "        #  tagsonly 只收集 ne_tag\n",
    "        #     ('*', '*', 'O')\n",
    "        #     ('*', 'O', 'O')\n",
    "        #     ('O', 'O', 'I-GENE')\n",
    "            \n",
    "            for i in range(2, self.n+1): #Count NE-tag 2-grams..n-grams\n",
    "                self.ngram_counts[i-1][tagsonly[-i:]] += 1\n",
    "            if ngram[-1][0] is not None: # If this is not the last word in a sentence\n",
    "                self.ngram_counts[0][tagsonly[-1:]] += 1 # count 1-gram\n",
    "                self.emission_counts[ngram[-1]] += 1 # and emission frequencies\n",
    "\n",
    "            # Need to count a single n-1-gram of sentence start symbols per sentence\n",
    "            if ngram[-2][0] is None: # this is the first n-gram in a sentence\n",
    "                self.ngram_counts[self.n - 2][tuple((self.n - 1) * [\"*\"])] += 1\n",
    "#             print (self.emission_counts)\n",
    "\n",
    "\n",
    "    def write_counts(self, output, printngrams=[1,2,3]):\n",
    "        \"\"\"\n",
    "        Writes counts to the output file object.\n",
    "        Format:\n",
    "\n",
    "        \"\"\"\n",
    "        # First write counts for emissions\n",
    "        for word, ne_tag in self.emission_counts:            \n",
    "            output.write(\"%i WORDTAG %s %s\\n\" % (self.emission_counts[(word, ne_tag)], ne_tag, word))\n",
    "\n",
    "\n",
    "        # Then write counts for all ngrams\n",
    "        for n in printngrams:            \n",
    "            for ngram in self.ngram_counts[n-1]:\n",
    "                ngramstr = \" \".join(ngram)\n",
    "                output.write(\"%i %i-GRAM %s\\n\" %(self.ngram_counts[n-1][ngram], n, ngramstr))\n",
    "\n",
    "    def read_counts(self, corpusfile):\n",
    "\n",
    "        self.n = 3\n",
    "        self.emission_counts = defaultdict(int)\n",
    "        self.ngram_counts = [defaultdict(int) for i in range(self.n)]\n",
    "        self.all_states = set()\n",
    "\n",
    "        for line in corpusfile:\n",
    "            parts = line.strip().split(\" \")\n",
    "            count = float(parts[0])\n",
    "            if parts[1] == \"WORDTAG\":\n",
    "                ne_tag = parts[2]\n",
    "                word = parts[3]\n",
    "                self.emission_counts[(word, ne_tag)] = count\n",
    "                self.all_states.add(ne_tag)\n",
    "            elif parts[1].endswith(\"GRAM\"):\n",
    "                n = int(parts[1].replace(\"-GRAM\",\"\"))\n",
    "                ngram = tuple(parts[2:])\n",
    "                self.ngram_counts[n-1][ngram] = count\n",
    "        \n",
    "    \n",
    "    def get_emission_paras(self):\n",
    "    \n",
    "        self.emision_parameters = defaultdict(float)\n",
    "        for wordNtag,counts in self.emission_counts.items():\n",
    "            tag = wordNtag[1] #get tag of the tuple\n",
    "            tagCount = self.ngram_counts[0][(tag,)]\n",
    "            emission = counts / tagCount\n",
    "            self.emision_parameters[wordNtag]=emission\n",
    "            \n",
    "            \n",
    "#             print (wordNtag,emission)\n",
    "#         print (self.emision_parameters)\n",
    "            \n",
    "    def replaceRare(self,training_file,trainingChange_file):\n",
    "        for wordNtag,counts in self.emission_counts.items():\n",
    "            if counts < 5:\n",
    "                self.rare.append(wordNtag[0])\n",
    "                \n",
    "#         print (self.rare)\n",
    "        \n",
    "        l = training_file.readline()\n",
    "        i = 0\n",
    "        while l:\n",
    "            if i%1000==0: \n",
    "                i= i+ 1 \n",
    "                print ('Now line',i)\n",
    "                \n",
    "            line = l.strip()\n",
    "            if line: # Nonempty line\n",
    "                \n",
    "                fields = line.split(\" \")\n",
    "                ne_tag = fields[-1] # tag\n",
    "                word = fields[0]\n",
    "                if word in self.rare:\n",
    "                    newline=\"%s %s\\n\" % ('_RARE_',ne_tag)\n",
    "#                     print (newline)\n",
    "                    trainingChange_file.write(newline)\n",
    "                else:\n",
    "#                     print (l)\n",
    "                    trainingChange_file.write(l)\n",
    "\n",
    "            else: # Empty line\n",
    "#                 print (\"Empty Line\")\n",
    "                trainingChange_file.write('\\n')\n",
    "        l = training_file.readline()\n",
    "\n",
    "        \n",
    "\n",
    "                \n",
    "        \n",
    "        trainingChange_file.close()\n",
    "\n",
    "\n",
    "    def get_argmax(self):\n",
    "#         buid argmax list\n",
    "        for wordNtag,values in self.emision_parameters.items():\n",
    "#             if wordNtag[0]=='_RARE_':\n",
    "                \n",
    "            self.argmax_list[wordNtag[0]].append((wordNtag[1],values))\n",
    "#         build argmax real tag\n",
    "        for word,values in self.argmax_list.items():\n",
    "            tag = max(values,key=itemgetter(1))[0]\n",
    "            self.argmax_value[word]=tag\n",
    "            \n",
    "        print(self.argmax_value)\n",
    "        \n",
    "    def write_prediction(self,testing,output):\n",
    "        # read in word first\n",
    "        for line in testing:\n",
    "            word = line.strip()\n",
    "            if word:\n",
    "                tag=self.argmax_value[word]\n",
    "                if tag:\n",
    "                    output.write(\"%s %s\\n\" % (word, tag))\n",
    "                else:\n",
    "                    tag=self.argmax_value['_RARE_']\n",
    "#                     print ('New word',tag)\n",
    "                    output.write(\"%s %s\\n\" % (word, tag))\n",
    "            else:\n",
    "                output.write(\"\\n\")\n",
    "        \n",
    "                \n",
    "#         # write (word tag) \n",
    "#         for word, tag in self.argmax_value.items():            \n",
    "#             output.write(\"%s %s\\n\" % (word, tag))\n",
    "        print (\"Finish!\")\n",
    "        output.close()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def usage():\n",
    "    print (\"\"\"\n",
    "    python count_freqs.py [input_file] > [output_file]\n",
    "        Read in a gene tagged training input file and produce counts.\n",
    "    \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    try:\n",
    "#         input = file(sys.argv[1],\"r\")\n",
    "        input = open(\"gene.counts\",'r')\n",
    "        training = open(\"gene.train\",'r')\n",
    "        trainingC = open(\"gene.train.replaced\",'w')\n",
    "        \n",
    "        testing = open('gene.test','r')\n",
    "        prediction = open(\"gene_test.p1.out\",'w');\n",
    "        \n",
    "    except IOError:\n",
    "        sys.stderr.write(\"ERROR: Cannot read inputfile %s.\\n\" % arg)\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Initialize a trigram counter\n",
    "    counter = Hmm(3)\n",
    "    # Collect counts\n",
    "#     counter.train(input)\n",
    "    \n",
    "    #Q1a: Read the counts\n",
    "    counter.read_counts(input)\n",
    "    counter.get_emission_paras()\n",
    "    \n",
    "    #Q1b: Replace\n",
    "    counter.replaceRare(training,trainingC)\n",
    "\n",
    "#     Q1c: Argmax\n",
    "#     counter.get_argmax()\n",
    "#     counter.write_prediction(testing,prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 47\n",
    "# down vote\n",
    "# accepted\n",
    "# Use max():\n",
    "\n",
    " \n",
    "# Using itemgetter():\n",
    "\n",
    "# In [53]: lis=[(101, 153), (255, 827), (361, 961)]\n",
    "\n",
    "# In [81]: from operator import itemgetter\n",
    "\n",
    "# In [82]: max(lis,key=itemgetter(1))[0]    #faster solution\n",
    "# Out[82]: 361"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
